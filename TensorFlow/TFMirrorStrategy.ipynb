{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffdca58-d7d9-49a1-b6b9-83178b12f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 02:56:33.613669: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756173393.636010 2043406 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756173393.643500 2043406 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756173393.660640 2043406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756173393.660668 2043406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756173393.660670 2043406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756173393.660672 2043406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-26 02:56:33.667076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You use TensorFlow v.2.19.0.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  # Or 'WARNING'\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "print(f'You use TensorFlow v.{tf.__version__}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb63525-9a75-4d44-85f2-46824a7610c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(\"Error setting GPU memory growth:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceafe9e5-0658-41cc-a139-d73286f7ed94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756173397.320699 2043406 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 878 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:09:00.0, compute capability: 8.9\n",
      "I0000 00:00:1756173397.323130 2043406 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 881 MB memory:  -> device: 1, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:42:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Path to store tensorflow datasets\n",
    "data_dir = '/data/tensorflow_datasets'\n",
    "\n",
    "# Load MNIST, store it in the specified folder\n",
    "datasets, info = tfds.load(\n",
    "    name='mnist',\n",
    "    data_dir=data_dir,       \n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9616bd4e-2dd3-4ef0-bd4b-49bc34460d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices the network trains on: 2\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f'Number of devices the network trains on: {strategy.num_replicas_in_sync}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b75ab9e-5ec7-4551-a2ea-2dad16213a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset summary:\n",
      "  Total samples: 70000.\n",
      "  Training samples: 60000.\n",
      "  Test samples: 10000.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of sample per split\n",
    "num_train_examples = info.splits['train'].num_examples  \n",
    "num_test_examples = info.splits['test'].num_examples   \n",
    "total_examples = info.splits.total_num_examples       \n",
    "\n",
    "# Print dataset summary\n",
    "print(\n",
    "    f'Dataset summary:\\n'\n",
    "    f'  Total samples: {total_examples}.\\n'\n",
    "    f'  Training samples: {num_train_examples}.\\n'\n",
    "    f'  Test samples: {num_test_examples}.'\n",
    ")\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10_000  # For shuffling\n",
    "\n",
    "# Define buffer and batch size for training\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "\n",
    "# Global batch size for multi-GPU. Adjust buffer size for distributed training if using MirroredStrategy\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ef5f3a-23e7-42b3-98ec-432f9aa201fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = (\n",
    "    mnist_train\n",
    "    .map(scale)\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "eval_dataset = (\n",
    "    mnist_test\n",
    "    .map(scale)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53363bda-a2b0-483a-a7ec-23e50048bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8fb241-ce9e-4514-ac12-dcb22706db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1756173398.276204 2043406 auto_shard.cc:558] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2025-08-26 02:56:38.451593: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 02:56:38.987791: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-08-26 02:56:38.989692: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "I0000 00:00:1756173400.215270 2043575 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1756173400.215278 2043573 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-08-26 02:56:41.336709: E tensorflow/core/common_runtime/base_collective_executor.cc:248] BaseCollectiveExecutor::StartAbort UNKNOWN: Error invoking NCCL: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\n",
      "2025-08-26 02:56:41.344489: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: Collective ops is aborted by: Error invoking NCCL: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\n",
      "The error could be from a previous operation. Restart your program to reset.\n",
      "\t [[{{function_node __inference_one_step_on_data_1820}}{{node adam/CollectiveReduceV2}}]] [type.googleapis.com/tensorflow.DerivedStatus='']\n",
      "2025-08-26 02:56:41.344551: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6224634787480558566\n",
      "2025-08-26 02:56:41.344564: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 1487294051110687276\n",
      "2025-08-26 02:56:41.843358: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: Collective ops is aborted by: Error invoking NCCL: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\n",
      "The error could be from a previous operation. Restart your program to reset.\n",
      "\t [[{{function_node __inference_one_step_on_data_1820}}{{node adam/CollectiveReduceV2_1}}]] [type.googleapis.com/tensorflow.DerivedStatus='']\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node adam/CollectiveReduceV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_2043406/1205105879.py\", line 1, in <module>\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\nCollective ops is aborted by: Error invoking NCCL: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\nThe error could be from a previous operation. Restart your program to reset.\n\t [[{{node adam/CollectiveReduceV2}}]] [Op:__inference_multi_step_on_iterator_1927]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnknownError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mUnknownError\u001b[39m: Graph execution error:\n\nDetected at node adam/CollectiveReduceV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_2043406/1205105879.py\", line 1, in <module>\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/data/PythonEnvsSSD/tf_gpu_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\nCollective ops is aborted by: Error invoking NCCL: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\nThe error could be from a previous operation. Restart your program to reset.\n\t [[{{node adam/CollectiveReduceV2}}]] [Op:__inference_multi_step_on_iterator_1927]"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9fa2f-2b82-4a87-862a-c5908aa78398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630ccb2-91c5-472e-8c1c-c4347b1371c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
